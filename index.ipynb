{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODI Queensland workshop - Web Scraping \n",
    "\n",
    "## QUT DMRC - 2015\n",
    "\n",
    "Patrik Wikstr√∂m & Brenda Moon\n",
    "\n",
    "### Welcome to the scraping the web workshop. \n",
    "\n",
    "During this session you will learn how to extract data from all those websites that do not provide structured access to their data via an API. This is a hands-on session where you will be introduced to a set of techniques and tools that will enable you to build a simple web crawler that efficiently collects large amounts of data (texts, images) from such websites. You will also learn how to store that data in a format suitable for subsequent processing and analysis. Note that this workshop does not require you to have any previous knowledge of programming or web technologies.\n",
    "\n",
    "We will use small steps in a series of notebooks to build up a script that gets content from the road signs section of the Queensland Government Transport and Motoring website (http://www.qld.gov.au/transport/safety/signs/index.html), extracts the fields we are interested in, and then stores the results.\n",
    "\n",
    "### Notebooks\n",
    "\n",
    "* Step 1. [Extract road sign name from a single item on a single page](step1.ipynb)\n",
    "* Step 2. [Extract all road sign names on a single page](step2.ipynb)\n",
    "* Step 3. [Extract all road sign data from a single page](step3.ipynb)\n",
    "* Step 4. [Structure the data extraction as a function](step4.ipynb)\n",
    "* Step 5. [Store the data in a dataframe and save to disk](step5.ipynb)\n",
    "* Step 6. [Restructure the code for clarity](step6.ipynb)\n",
    "* Step 7. [Plotting, tiny stat analysis and improved I/O](step7.ipynb)\n",
    "* Final. [Support for multiple pages](final.ipynb)\n",
    "\n",
    "\n",
    "### Python modules used in this workshop\n",
    "\n",
    "We use three Python modules in this workshop. The full documentation for each is available on their websites:\n",
    "\n",
    "* [Requests](http://docs.python-requests.org/en/latest/) - get webpages from urls\n",
    "* [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) - select text out of webpages\n",
    "* [Pandas](http://pandas.pydata.org/) - python data analysis library\n",
    "\n",
    "These pages are in [Jupyter Notebook](https://jupyter.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
